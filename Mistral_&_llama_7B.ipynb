{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDMC-v-k_yFX"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip -q install sentence-transformers\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.44 --force-reinstall --no-cache-dir\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index==0.9.47\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "# !pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "g0pouZaFAEB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-2lZLcQBrtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlAUGwz6Brq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model With mistral_7B"
      ],
      "metadata": {
        "id": "oPfwsPN00k8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# import sys\n",
        "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "# from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "# from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "# from llama_index.llms.llama_cpp import LlamaCPP\n",
        "# from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt"
      ],
      "metadata": {
        "id": "8h-J_6m0kvik"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "# documents = SimpleDirectoryReader(\"/content/new_data\").load_data()\n",
        "\n",
        "# llm = LlamaCPP(\n",
        "# model_url='https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q3_K_M.gguf',\n",
        "# # model_path=r\"C:\\Users\\Shivanshu.dwivedi\\Shivanshu\\Desktop\\AI_memory\\re-memory-engine\\chatbot_logic\\model_mistral\\mistral-7b-instruct-v0.1.Q3_K_M.gguf\",\n",
        "# temperature=0.1,\n",
        "# max_new_tokens=256,\n",
        "# context_window=3900,\n",
        "# generate_kwargs={},\n",
        "# model_kwargs={\"n_gpu_layers\": -1},\n",
        "# messages_to_prompt=messages_to_prompt,\n",
        "# completion_to_prompt=completion_to_prompt,\n",
        "# verbose=True,\n",
        "# )"
      ],
      "metadata": {
        "id": "HvszhBGZkAN8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RUA4WIN009v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bT9ewKWZ01Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVyOvuSP01DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsGAkXSQ01GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNIl1mMv01I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUgZ2WyP01K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model With Llama-2-7B"
      ],
      "metadata": {
        "id": "zgMClLzj01NO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "from llama_index.llms import LlamaCPP\n",
        "import time"
      ],
      "metadata": {
        "id": "kS4d5kfsx4uY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/new_data\").load_data()\n",
        "\n",
        "# Specify the path where you want to save the model\n",
        "model_path = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q3_K_M.gguf\"\n",
        "\n",
        "# Download the model with retries\n",
        "max_retries = 3\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        llm = LlamaCPP(\n",
        "            model_url = model_path,\n",
        "            temperature=0.1,\n",
        "            max_new_tokens=256,\n",
        "            context_window=3900,\n",
        "            generate_kwargs={},\n",
        "            model_kwargs={\"n_gpu_layers\": -1},\n",
        "            messages_to_prompt=messages_to_prompt,\n",
        "            completion_to_prompt=completion_to_prompt,\n",
        "            verbose=True,\n",
        "        )\n",
        "        break  # Exit the loop if the download is successful\n",
        "    except ValueError:\n",
        "        if i < max_retries - 1:\n",
        "            print(f\"Download attempt {i+1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Raise the exception if all retries fail"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RARHKVj8x9D4",
        "outputId": "d8c8f420-4f3b-4c7f-f934-53b5a7905823"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading url https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q3_K_M.gguf to path /tmp/llama_index/models/llama-2-7b-chat.Q3_K_M.gguf\n",
            "total size (MB): 3298.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3146it [00:27, 113.29it/s]                          \n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /tmp/llama_index/models/llama-2-7b-chat.Q3_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 12\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q3_K:  129 tensors\n",
            "llama_model_loader: - type q4_K:   92 tensors\n",
            "llama_model_loader: - type q5_K:    4 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    53.71 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3900\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1950.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1950.00 MiB, K (f16):  975.00 MiB, V (f16):  975.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    16.64 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = LangchainEmbedding(\n",
        "HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=256,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "Gsl5877zkS1z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "N8dT_ot_kWdR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"what are variables\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFZDEG6WkXty",
        "outputId": "1ad9abb0-ae4c-42f1-8682-706aade3f486"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     497.54 ms\n",
            "llama_print_timings:      sample time =      80.90 ms /   152 runs   (    0.53 ms per token,  1878.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     497.01 ms /   349 tokens (    1.42 ms per token,   702.20 tokens per second)\n",
            "llama_print_timings:        eval time =    4866.17 ms /   151 runs   (   32.23 ms per token,    31.03 tokens per second)\n",
            "llama_print_timings:       total time =    5928.36 ms /   500 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the provided context information, variables in Python are data storage locations that hold values. In Python, variables are dynamically typed, meaning you don't need to declare their type explicitly. This means that you can assign different data types to the same variable without any issues. For example, you can assign an integer value to a variable and then later assign a string value to the same variable without any errors.\n",
            "In Python, variables are used to store and manipulate data within your programs. They allow you to store values that you can use throughout your code, making it easier to organize and structure your programs. Without variables, you would have to hardcode values directly into your code, which can make your code more difficult to read and maintain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zuUsOgJj0d92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}